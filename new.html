<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>IDOT Automatic Contract Scraper (Pyfetch)</title>
    
    <!-- Tailwind CSS for Styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    
    <!-- Pyodide CDN -->
    <!-- Pyodide is used for robust HTML parsing (BeautifulSoup) and CSV generation, and network requests (pyfetch). -->
    <script src="https://cdn.jsdelivr.net/pyodide/v0.25.0/full/pyodide.js"></script>

    <!-- Custom Config for Corporate Theme -->
    <script>
        tailwind.config = {
            theme: {
                extend: {
                    colors: {
                        brand: {
                            black: '#0a0a0a',
                            dark: '#1f1f1f',
                            gray: '#e5e5e5',
                            light: '#f9f9f9',
                        }
                    },
                    fontFamily: {
                        sans: ['Inter', 'system-ui', 'sans-serif'],
                    }
                }
            }
        }
    </script>

    <style>
        /* Custom Utilities */
        .loader {
            border: 3px solid #f3f3f3;
            border-top: 3px solid #0a0a0a; /* Brand Black */
            border-radius: 50%;
            width: 20px;
            height: 20px;
            animation: spin 1s linear infinite;
            display: inline-block;
            vertical-align: middle;
        }

        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        /* Enforce smooth scrolling for layout shifts */
        html { scroll-behavior: smooth; }
    </style>
</head>
<body class="bg-brand-light min-h-screen p-4 sm:p-8">

    <div class="max-w-4xl mx-auto bg-white shadow-xl rounded-xl p-6 sm:p-10 border border-brand-gray">
        <header class="mb-8 border-b pb-4">
            <h1 class="text-3xl font-extrabold text-brand-black mb-2">IDOT Automatic Contract Scraper</h1>
            <p class="text-brand-dark">Uses Pyodide's built-in <code>pyfetch</code> (as the original intended) to perform the automatic two-step scrape.</p>
        </header>

        <section id="input-section" class="space-y-6">
            <div id="repo-input-group">
                <label for="repoUrlInput" class="block text-sm font-medium text-brand-black mb-2">
                    Step 1: Repository Site URL (Pyfetch)
                </label>
                <input type="url" id="repoUrlInput" placeholder="e.g., https://webapps1.dot.illinois.gov/WCTB/NoticeOfLetting" 
                       class="w-full p-3 border border-brand-gray rounded-lg focus:ring-2 focus:ring-brand-black focus:border-brand-black" required>
                <p class="mt-2 text-xs text-gray-500">Paste the URL of the IDOT Letting Notice page (the source page with the table).</p>
            </div>
            
            <button id="scrapeBtn" 
                    class="w-full sm:w-auto px-6 py-3 bg-brand-black text-white font-semibold rounded-lg shadow-md hover:bg-gray-800 transition duration-150 disabled:opacity-50 flex items-center justify-center"
                    disabled>
                <span id="button-text">Load Contracts and Scrape Details</span>
                <span id="spinner" class="loader ml-3 hidden"></span>
            </button>
        </section>

        <section id="output-section" class="mt-8">
            <h2 class="text-xl font-semibold text-brand-black mb-3">Scraping Status</h2>
            <div id="log-output" class="bg-brand-gray p-4 rounded-lg h-40 overflow-y-auto text-sm font-mono text-brand-dark">
                Awaiting input...
            </div>
            
            <button id="downloadBtn" 
                    class="mt-6 w-full sm:w-auto px-6 py-3 bg-green-600 text-white font-semibold rounded-lg shadow-md hover:bg-green-700 transition duration-150 hidden"
                    title="Download the combined contract data as a CSV file">
                Download Final CSV
            </button>
        </section>
        
        <footer class="mt-10 pt-4 border-t text-xs text-gray-500 text-center">
            Parsing and network logic powered by Python's Pyodide (`pyfetch` and BeautifulSoup).
        </footer>
    </div>

    <script type="text/python" id="python-code">
import sys
import js
import json
import csv
from io import StringIO
from bs4 import BeautifulSoup

# Import pyfetch from Pyodide
from pyodide.http import pyfetch 

# --- Configuration ---

# Base URL for contract details pages. This is used to complete the relative links found in the table.
BASE_URL = "https://webapps1.dot.illinois.gov" 

# Target Counties for filtering
TARGET_COUNTIES = {
    "Boone", "Cook", "Grundy", "DuPage", "Kane", "Kendall", "Lake", "McHenry", "Will", "Various"
}

# Target Status for filtering
TARGET_STATUSES = {
    "Active", "Awarded"
}

# CSV output header structure (must match output_new_scrape.csv)
FINAL_CSV_HEADERS = [
    "Item-Contract", "Status", "Status Date", "Region", "District", "Counties", 
    "Bulletin Description", "", "Letting_Date", "Contract_ID", "Contractor_Number", 
    "Contractor_Name", "Award_Price", "County", "Section", "Federal_Project_No", "Source_URL"
]

# --- Network and Parsing Functions ---

async def fetch_html_py(url):
    """Fetches HTML content using pyfetch and handles errors."""
    try:
        js.logToUI(f"Fetching URL: {url}", 'info')
        # Use headers to mimic a browser and increase success rate
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/100.0.4896.75 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8'
        }
        
        # Use pyfetch for the network request
        response = await pyfetch(url, headers=headers)
        
        if response.status != 200:
            raise Exception(f"HTTP Error {response.status}: Failed to retrieve content from {url}")
            
        return await response.text()
        
    except Exception as e:
        js.logToUI(f"Network error during pyfetch: {e}", 'error')
        raise

def parse_detail_page(detail_html_str):
    """
    Parses the HTML content of a single contract detail page to extract 
    Contractor details, Award Price, and other secondary data points.
    Returns the extracted data as a JSON string.
    """
    soup = BeautifulSoup(detail_html_str, 'html.parser')
    
    data = {
        "Contract_ID": "",
        "Contractor_Number": "",
        "Contractor_Name": "",
        "Award_Price": "",
        "County": "",
        "Section": "",
        "Federal_Project_No": "",
    }

    main_div = soup.find('div', class_='col-md-9')
    if not main_div:
        js.logToUI("Warning: Could not find main content div on detail page.", 'warning')
        return json.dumps(data)

    # 1. Scrape Contract, Section, Federal Project Number from the top info block
    info_table = main_div.find('table', class_='table')
    if info_table:
        for row in info_table.find_all('tr'):
            cols = row.find_all(['th', 'td'])
            if len(cols) == 2:
                key = cols[0].get_text(strip=True).replace(':', '')
                value = cols[1].get_text(strip=True)
                
                if 'Contract ID' in key:
                    data['Contract_ID'] = value
                elif 'Section' in key:
                    data['Section'] = value
                elif 'Federal Project No' in key:
                    data['Federal_Project_No'] = value

    # 2. Scrape Contractor details and Award Price
    contractor_header = soup.find('h3', string=lambda t: t and 'Contractor Information' in t)
    if contractor_header:
        contractor_table = contractor_header.find_next_sibling('table')
        if contractor_table:
            for row in contractor_table.find_all('tr'):
                cols = row.find_all(['th', 'td'])
                if len(cols) == 2:
                    key = cols[0].get_text(strip=True).replace(':', '')
                    value = cols[1].get_text(strip=True)
                    
                    if 'Contractor Number' in key:
                        data['Contractor_Number'] = value
                    elif 'Contractor Name' in key:
                        data['Contractor_Name'] = value
                    elif 'Award Price' in key:
                        data['Award_Price'] = value.split('(')[0].strip()
                        
    return json.dumps(data)

def scrape_repository_table(repo_html_str):
    """
    Parses the Repository HTML, extracts and filters all contract links.
    Returns a JSON string of initial contract data and URLs.
    """
    soup = BeautifulSoup(repo_html_str, 'html.parser')
    js.logToUI("Parsing repository table and applying filters...", 'info')

    data_table = soup.find('table', class_='table')
    if not data_table:
        raise ValueError("Could not find the main data table. Page structure may have changed.")

    ITEM_CONTRACT_COL = 0
    STATUS_COL = 1
    COUNTIES_COL = 5
    
    # Scrape the common Letting Date
    letting_date = "N/A"
    letting_header = soup.find('h1')
    if letting_header and 'Letting' in letting_header.get_text():
        letting_date = letting_header.get_text(strip=True)
    
    if letting_date == "N/A":
         letting_date_element = soup.find('div', class_='page-header')
         if letting_date_element:
             text = letting_date_element.get_text(strip=True)
             if 'Letting' in text:
                 letting_date = ' '.join(text.split()).split('\n')[0].strip()
    
    results = []
    total_rows = 0
    filtered_count = 0

    tbody = data_table.find('tbody')
    if not tbody:
         raise ValueError("Could not find table body (tbody) element.")

    for row in tbody.find_all('tr'):
        total_rows += 1
        cols = row.find_all('td')
        
        if len(cols) < 8: 
            continue
            
        item_contract_cell = cols[ITEM_CONTRACT_COL]
        status = cols[STATUS_COL].get_text(strip=True)
        status_date = cols[2].get_text(strip=True)
        region = cols[3].get_text(strip=True)
        district = cols[4].get_text(strip=True)
        counties = cols[COUNTIES_COL].get_text(strip=True)
        bulletin_desc = cols[7].get_text(strip=True)
        
        a_tag = item_contract_cell.find('a', href=True)
        relative_url = a_tag['href'] if a_tag else None
        
        if not relative_url:
            js.logToUI(f"Skipping row {total_rows}: No Item-Contract URL found.", 'warning')
            continue

        # --- Apply Filtering Criteria ---
        is_county_match = any(c.strip() in TARGET_COUNTIES for c in counties.split(','))
        is_status_match = status in TARGET_STATUSES
        
        if is_county_match and is_status_match:
            full_url = BASE_URL + relative_url
            filtered_count += 1
            
            contract_data = {
                "Item-Contract": item_contract_cell.get_text(strip=True),
                "Status": status,
                "Status Date": status_date,
                "Region": region,
                "District": district,
                "Counties": counties,
                "Bulletin Description": bulletin_desc,
                "Letting_Date": letting_date,
                "Source_URL": full_url,
                "Detail_URL": full_url # URL used for the second step fetch
            }
            results.append(contract_data)

    js.logToUI(f"Repository Scrape Complete: Found {total_rows} total contracts. {filtered_count} contracts match criteria. Starting detail fetch via Python...", 'success')
    return json.dumps(results)

async def perform_full_scrape(repo_url):
    """
    Main Python entry point to handle the entire scraping workflow.
    1. Fetch repository HTML.
    2. Scrape repository table and filter contracts.
    3. Iterate, fetch detail HTML for each contract.
    4. Parse detail data and enrich contracts.
    5. Generate final CSV.
    """
    # 1. Fetch Repository HTML
    repo_html = await fetch_html_py(repo_url)

    # 2. Scrape repository table and filter
    contracts_json = scrape_repository_table(repo_html)
    contracts = json.loads(contracts_json)

    if not contracts:
        return json.dumps({"csv": None, "message": "No contracts matched the filtering criteria."})

    # 3. Fetch and Parse Detail Pages
    js.logToUI(f"Starting detailed scrape for {len(contracts)} contracts...", 'info')

    successful_scrapes = 0
    for i, contract in enumerate(contracts):
        js.logToUI(f"Scraping {i + 1}/{len(contracts)}: {contract['Item-Contract']}", 'info')
        
        try:
            # Fetch the detail page HTML using pyfetch
            detail_html = await fetch_html_py(contract['Detail_URL'])
            
            # Parse detail HTML and get the enriched data object (as JSON string)
            detail_data_json = parse_detail_page(detail_html)
            detail_data = json.loads(detail_data_json)
            
            # Merge the new data into the contract object
            contract.update(detail_data)
            successful_scrapes += 1

        except Exception as e:
            js.logToUI(f"Warning: Detail scrape failed for {contract['Item-Contract']}. Skipping. Error: {e}", 'warning')
            # The contract remains in the list with missing detail fields, as intended

    js.logToUI(f"Detail Scrape Complete. Successfully processed {successful_scrapes} contracts.", 'success')
    
    # 5. Generate Final CSV
    js.logToUI("Generating final CSV file...", 'info')
    output = StringIO()
    
    # Define headers used for writing (including the empty column placeholder)
    headers_to_write = FINAL_CSV_HEADERS[:7] + [""] + FINAL_CSV_HEADERS[8:]
    writer = csv.DictWriter(output, fieldnames=headers_to_write, extrasaction='ignore', quoting=csv.QUOTE_MINIMAL)
    
    writer.writeheader()
    writer.writerows(contracts)
    
    csv_content = output.getvalue()
    js.logToUI("CSV Generation Complete.", 'success')

    # Return the result as a JSON string containing the CSV content
    return json.dumps({"csv": csv_content, "message": "Scraping complete."})


# Export the main scraping function to the JavaScript scope
pyodide_globals = globals()
pyodide_globals['perform_full_scrape'] = perform_full_scrape

    </script>
    
    <script>
        let csvContent = null;
        
        // Expose a helper function for Python to log back to the UI
        function logToUI(message, type = 'info') {
            const logElement = document.getElementById('log-output');
            const colorClass = {
                'info': 'text-brand-dark',
                'success': 'text-green-600 font-bold',
                'warning': 'text-yellow-600',
                'error': 'text-red-600 font-bold'
            }[type];
            
            const timestamp = new Date().toLocaleTimeString();
            const logEntry = document.createElement('p');
            logEntry.className = `p-1 ${colorClass}`;
            logEntry.textContent = `[${timestamp}] ${message}`;
            logElement.appendChild(logEntry);
            
            // Scroll to the bottom
            logElement.scrollTop = logElement.scrollHeight;
        }

        // Global element references for easy access in JS and Python log function
        const els = {
            repoUrlInput: document.getElementById('repoUrlInput'),
            scrapeBtn: document.getElementById('scrapeBtn'),
            downloadBtn: document.getElementById('downloadBtn'),
            spinner: document.getElementById('spinner'),
            buttonText: document.getElementById('button-text')
        };
        
        // Expose elements and functions to Python's scope for control
        window.els = els;
        window.logToUI = logToUI;

        // Pyodide initialization function (FIXED)
        async function initPyodide() {
            try {
                logToUI("Initializing Python runtime (Pyodide)...", 'info');
                
                // 1. Load Pyodide Core and assign it to the global scope
                window.pyodide = await loadPyodide();
                
                // 2. Install Packages
                await window.pyodide.loadPackage("micropip");
                await window.pyodide.runPythonAsync("import micropip");
                // Install BeautifulSoup4
                await window.pyodide.runPythonAsync("await micropip.install('beautifulsoup4')");
                
                // 3. Load the rest of the Python script (which now only contains functions)
                // We use runPython to load the functions into the global Python scope
                window.pyodide.runPython(document.getElementById('python-code').textContent);

                logToUI("Python runtime ready. Enter the Repository URL.", 'success');
                els.scrapeBtn.disabled = false;
                els.scrapeBtn.innerText = "Load Contracts and Scrape Details";

            } catch (err) {
                console.error("Pyodide Initialization Failed:", err);
                logToUI(`CRITICAL: Pyodide initialization failed. Check console for details.`, 'error');
                els.scrapeBtn.disabled = true;
                els.scrapeBtn.innerText = "Error Loading Tool";
            }
        }


        // Handle Scrape Button Click
        els.scrapeBtn.addEventListener('click', async () => {
            const repoUrl = els.repoUrlInput.value.trim();
            if (!repoUrl) {
                logToUI("Please enter a valid Repository URL.", 'warning');
                return;
            }

            // UI Scrape Start State
            els.scrapeBtn.disabled = true;
            els.repoUrlInput.disabled = true;
            els.downloadBtn.classList.add('hidden');
            els.spinner.classList.remove('hidden');
            els.buttonText.textContent = "Working...";
            logToUI("Starting two-step scraping process...", 'info');
            
            csvContent = null;
            
            try {
                // Get the main Python entry point function
                const pythonScrapeFunc = window.pyodide.globals.get('perform_full_scrape');
                
                // Call the Python function, which handles both fetches and parsing.
                // Because it's an async Python function, we use the JS proxy directly.
                const resultJson = await pythonScrapeFunc(repoUrl);
                const result = JSON.parse(resultJson);

                if (result.csv) {
                    csvContent = result.csv;
                    logToUI(result.message, 'success');
                    
                    // UI Completion State
                    els.scrapeBtn.classList.add('hidden');
                    els.downloadBtn.classList.remove('hidden');

                } else {
                    logToUI(result.message, 'warning');
                    els.scrapeBtn.classList.remove('hidden');
                }
                
            } catch (err) {
                // Error handling from Python or JS execution
                const errorMessage = err.message || String(err);
                console.error("Scraping Error:", err);
                logToUI(`Critical Error: ${errorMessage}`, 'error');
                
                // Show the scrape button again for retry
                els.scrapeBtn.classList.remove('hidden');
                
            } finally {
                // Reset UI state
                els.spinner.classList.add('hidden');
                els.buttonText.textContent = "Load Contracts and Scrape Details";
                els.repoUrlInput.disabled = false;
                els.scrapeBtn.disabled = false;
            }
        });

        // Handle Download
        els.downloadBtn.addEventListener('click', () => {
            if (!csvContent) {
                logToUI("No CSV content generated. Run the scrape first.", 'warning');
                return;
            }
            
            const blob = new Blob([csvContent], { type: 'text/csv;charset=utf-8;' });
            const url = URL.createObjectURL(blob);
            const link = document.createElement("a");
            
            const timestamp = new Date().toISOString().replace(/[:.]/g, '-').slice(0, 19);
            link.setAttribute("href", url);
            link.setAttribute("download", `enhanced_contract_data_${timestamp}.csv`);
            link.style.visibility = 'hidden';
            
            document.body.appendChild(link);
            link.click();
            document.body.removeChild(link);
        });

        // Start initialization
        window.addEventListener('load', initPyodide);

    </script>
</body>
</html>
